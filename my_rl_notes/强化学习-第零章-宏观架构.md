# 强化学习的宏观架构

在正式开始由于学习之前，我们必须先看懂这本书的“地图”（The Map of the Book）。赵世钰老师编写的这本书逻辑非常严密，它不是零散知识点的堆砌，而是一个层层递进的系统。

全书十章可以被清晰地划分为两个大板块：基础工具箱（Fundamental Tools）和算法演进（Algorithms/Methods）。

这种划分非常符合机器人学的工程逻辑：先有物理和数学基础（建模），再有控制与规划算法（求解）。

## 打造地基（基础工具）

这一部分涵盖了前三章，是理解 RL 灵魂的关键。如果没有这部分的**技术直觉**，后面的算法就只是一堆枯燥的符号。

### Chapter 1: 基本概念 (Basic Concepts)

这是 RL 的语言学，会有包括状态、动作、奖励、回报、策略等基础概念。在这里，我们定义什么是 Agent（机器人）、Environment（物理世界）、State（传感器状态）、Action（电机指令）以及最重要的 Reward（奖励）。这里会引入马尔可夫决策过程 (MDP)，它是描述机器人与环境交互的标准数学框架。要做的是理解这些概念如何对应到你实际的机器人系统中。

### Chapter 2: 贝尔曼方程 (Bellman Equation)

这章介绍了两个关键点：一个关键概念，一个关键工具。只要明白了这两个关键点，这一章的脉络就会非常清晰。

一个关键概念指的是状态值 (state value)。其定义为当智能体从某个状态出发时所能获得的回报的期望值。因为状态值越大说明对应的策略就越好，所以状态值可以用来评估一个策略的好坏，这是我们需要学习状态值的本质原因。

一个关键工具指的是贝尔曼方程 (Bellman equation)。用一句话来概述，贝尔曼方程描述了所有状态值之间的关系。通过求解贝尔曼方程，我们就可以得到状态值，这是我们需要学习贝尔曼方程的本质原因。求解状态值的过程称为策略评价，这是强化学习中的一个重要概念。最后，在介绍状态值的基础上，本章进一步介绍动作值的概念。

在机器人学中，我们需要评估一个策略好不好。状态值就是评估标准。贝尔曼方程用数学语言描述了“当前状态值”与“未来状态值”之间的递归关系。理解它，也就理解了 RL 中预测未来的本质——Policy Evaluation。

### Chapter 3: 贝尔曼最优方程 (Bellman Optimality Equation)

这里也介绍了两个关键点：一个关键概念，一个关键工具。只要明白了这两个关键点，这一章的脉络也会非常清晰。

一个关键概念指的是最优策略 (optimal policy)。最优策略的定义是该策略相比其他任意策略在所有状态上都具有更高的状态值。

一个关键工具指的是贝尔曼最优方程 (Bellman optimality equation)。顾名思义，贝尔曼最优方程是一种特殊的贝尔曼方程，之所以称其为“最优”，是因为这个方程对应了最优策略。

本章涉及一个非常基础的问题：强化学习的终极目标是什么？强化学习的终极目标是寻找最优策略。

贝尔曼最优方程之所以重要，是因为它可以用来刻画最优策略。虽然初学者可能需要花一些时间去理解这个方程，但是一旦理解，你就会发现它是一个十分优雅和强大的工具，能够帮助我们深入理解许多基本问题和算法。

## 理想条件下拥有模型时的求解 (Model-based)

打好地基后，从第 4 章开始，书本进入了算法实战部分。这部分的演进逻辑非常精彩，它实际上是在不断**放宽假设**，从“拥有完美上帝视角的理想实验”一步步走向“只有传感器数据的真实机器人世界”。

### Chapter 4: 动态规划与三种迭代

如果你拥有环境的完美数学模型（比如你知道机器人运动学的精确微分方程），你可以直接用动态规划算法（如值迭代 Value Iteration、策略迭代 Policy Iteration、截断策略迭代 truncated policy iteration）解出最优策略。

这实际上就是求解贝尔曼最优方程的数值方法。这在经典控制理论中很常见，但在复杂的 embodied AI 场景中，我们往往无法获得完美模型。

* 值迭代算法实际上就是第 3 章给出的用于求解贝尔曼最优方程的算法。
* 策略迭代算法是值迭代算法的推广，它也是第 5 章将介绍的蒙特卡罗方法的直接基础。
* 截断策略迭代算法是更加一般化的算法：值迭代和策略迭代是它的两个特殊情况。

这三种算法具有类似的结构，即每次迭代都有两个步骤：一个步骤用来更新值，另一个步骤用来更新策略。这种在更新值和更新策略之间不断切换的思想被称为广义策略迭代 (generalized policy iteration, GPI)，该思想广泛存在于各式各样的强化学习算法中。

## 抛弃非理想模型下拥抱数据的方法 (Model-free)

当你没有模型，只有机器人试错的经验数据 (Data/Experience) 时，RL 的威力才真正显现。

这里有一个知识鸿沟需要填补：怎么样在没有模型的情况下学习最优策略？

其基本思路非常简单：可以使用大量数据来估计状态值进而改进策略。如果没有模型，我们必须要有数据；如果没有数据，必须要有模型。如果两者都没有，那什么也做不了。“数据”在强化学习中指的是智能体与环境交互产生的经验样本 (experience sample)。

### Chapter 5: 蒙特卡洛方法 (Monte Carlo Methods)

这是最直观的“无模型”方法。就像去赌场扔骰子一样，通过大量的采样 (Sampling) 来估计价值。它不需要方程求解，而是靠统计平均。这章会介绍三种基于蒙特卡罗的算法，这些算法能够从经验样本中学习最优策略。

第一个 MC Basic 算法就是把第 4 章介绍的策略迭代算法中“需要模型”的模块替换成“不需要模型”的模块。理解 MC Basic 算法对于理解其他的蒙特卡罗方法的基本思想非常重要。通过扩展这一算法，我们可以得到更复杂但更高效的算法。

除此之外还有 exploration vs. exploitation（探索与利用）的经典权衡。但它的缺点是必须等一个回合 (Episode) 完全结束才能更新策略 (Non-incremental)，效率较低。

### Chapter 6: 随机逼近理论 (Stochastic Approximation)

这是一个数学加油站。为了从“回合更新”进化到“单步更新”，我们需要数学工具的支持。虽然不讲具体 RL 算法，但它是理解后续时序差分方法 TD Learning 为什么收敛的数学基石。

第 7 章中的时序差分更新 TD 算法是增量式的 (incremental)，而第 5 章中的算法则是非增量式的 (nonincremental)。如果不很好地填补这个鸿沟，那么读者很容易感到迷惑。因此，我们在第 6 章通过介绍随机近似 (stochastic approximation) 理论来填补这一知识鸿沟。

随机近似指的是用随机迭代算法求解方程或者优化问题的过程。经典的随机梯度下降算法和 Robbins-Monro 算法都是特殊的随机近似算法。尽管这一章没有介绍任何强化学习算法，但是它却十分重要，因为它为第 7 章奠定了必要的基础。

### Chapter 7: 时序差分方法 (Temporal-Difference Methods)

这是 RL 的精华所在，结合了蒙特卡洛（采样）和动态规划（自举/Bootstrapping）的优点。TD 算法（如经典的 Sarsa 和 Q-learning）是增量式 (Incremental) 的，机器人每走一步就能学习一步，不需要等任务结束。对于实时的机器人控制来说，这是极其重要的特性。

实际上，时序差分算法可以被视为求解贝尔曼方程或贝尔曼最优方程的随机近似算法。时序差分方法也是无模型的。由于其增量形式，它相比蒙特卡罗方法具有一定优势，例如它可以在线学习：每次接收到经验样本时，它可以立即用于更新值和策略。

本章将介绍一些具体的时序差分算法，包括 Sarsa 和 Q-learning 等。此外，同策略 (on-policy) 和异策略 (of-policy) 的概念也将在本章介绍。

## 规模化与现代方法 (Scaling Up)

### Chapter 8: 值函数近似 (Value Function Approximation)

**用函数表示状态值**

这一章仍然在介绍时序差分方法，只不过它采用了不同的方式来表示状态值和动作值。在本章之前，状态值和动作值都是通过表格表示的。虽然表格方法易于理解，但是难以高效处理大型状态或动作空间。为此，可以用函数来表示值。

理解这种方法的关键是理解其三个步骤：第一步是选择合适的目标函数，第二步是推导目标函数的梯度，第三步是用基于梯度的方法来优化目标函数。值函数方法很重要，因为现在它已经成为表示值的标准方法，而且这也是将人工神经网络作为函数逼近器引入强化学习的切入点。著名的深度 Q-learning 算法也将在本章介绍。

之前的章节都假设状态可以用表格 (Table) 存下来。但对于高维输入的机器人（比如输入是摄像头图像），表格存不下。这章引入了函数近似，也就是用神经网络来拟合价值函数（Deep Q-Learning 就在这里诞生）。这是深度强化学习 (Deep RL) 的入口。

### Chapter 9: 策略梯度 (Policy Gradient)

**用函数表示策略**

这是许多现代强化学习算法的基础。在本章之前，策略都是通过表格表示的，而本章采用函数来表示策略（注意第 8 章是用函数来表示值的）。

策略梯度方法的基本思想非常简单：第一，选择一个合适的目标函数；第二，求解目标函数对策略参数的梯度；第三，应用基于梯度的算法来优化目标函数。策略梯度方法有众多优势，例如它可以更加高效地处理大型状态和动作空间，也具有更强的泛化能力。

前面的方法都是基于“价值”的 (Value-based)，这章转向了基于“策略”的方法 (Policy-based)。我们不再间接计算价值，而是直接优化策略函数的参数。这对机器人控制至关重要，因为它能处理连续动作空间 (Continuous Action Space)，比如机械臂的力矩控制。

### Chapter 10: Actor-Critic 方法

从一个角度来说，演员-评论家方法是第 9 章中策略梯度方法和第 8 章中值函数方法的结合体。从另一个角度来说，演员-评论家方法仍然是第 9 章中的策略梯度方法，只不过其中估计值的时候使用了第 8 章中值函数的方法。因此，在学习第 10 章之前，需要先理解第 8 章和第 9 章的内容。

这是集大成者。它结合了 Policy-based (Actor) 和 Value-based (Critic) 的优点。Critic 负责打分，Actor 负责行动。这是目前最先进的机器人学习算法（如 PPO, SAC）的基础架构。