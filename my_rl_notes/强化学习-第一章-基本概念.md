<script setup>
import GridWorld from '../components/chapter1/GridWorld.vue'
import TransitionProb from '../components/chapter1/TransitionProb.vue'
import PolicyDist from '../components/chapter1/PolicyDist.vue'
import ReturnCalc from '../components/chapter1/ReturnCalc.vue'
</script>

# 强化学习-第一章：基本概念

本章介绍了强化学习的基本概念。这些概念在本书中被广泛使用。我们首先通过**网格世界**介绍这些概念，然后在**马尔可夫决策过程**的框架下对它们进行更加具体介绍。

![](./assets-of-强化学习-第一章-基本概念/chapter-1.png)
> 图 1.1 第一章位于强化学习的地图位置

## 1. 网格世界 (Grid World)

图 1.2 展示了一个智能体(agent)在网格世界中移动的例子，其将会在整本书中被广泛使用。在每个时刻，智能体只能占据一个单元格。白色单元格代表智能体可以进入的区域，蓝色单元格代表目标区域，橙色单元格代表智能体不可以进入的禁区。智能体的任务即为从初始区域出发，最终到达目标区域。

![](./assets-of-强化学习-第一章-基本概念/1.jpg)
> 图 1.2 智能体在网格世界中的移动过程。

如果智能体事先知道网格世界的地图，那么规划一条到达终点的路径其实不难。但现实世界中情况往往会很复杂，智能体很难了解有关环境的任何信息。此时便需要通过与环境交互获取经验，从而找到一个好的策略。因此在本章中，我们将要学习一系列基本概念来描述这样一个过程。
- 智能体之外的一切都被视为环境
- 智能体包含感知、决策、和执行机构

<GridWorld />

可以操控机器人，看它如何在网格世界中运动。设置机器人起始点、目标点和禁止点，设置奖励和折扣因子，切换 3 个固定的策略

## 2. 状态、动作与状态转移

### 状态 (State)

本书中首先介绍的概念是**状态** (state)，它描述了智能体相对于环境的状况。在网格世界的例子中，状态对应于智能体所在单元格的位置。如图 1.3 所示，这个网格具有9个单元格，因此也对应了9个状态，它们分别表示为 $s_1,s_2,...,s_9$，**所有状态的集合被叫做状态空间 (state space)**，表示为 $\mathcal{S}=\{s_{1},\ldots,s_{9}\}$。

### 动作 (Action)

我们将要介绍的第二个概念是**行动** (action)。具体到网格世界中，智能体在每个状态下会采取五个可能的行动：向上移动，向右移动，向下移动，向左移动和静止不动。这五个行动分别被表示为 $a_1,a_2,...,a_5$(如图 1.3(b) 中所示)。**所有动作的集合被称为行动空间 (action space)**，表示为 $\mathcal{A}=\{a_{1},\ldots,a_{5}\}$。

不同的状态可以有不同的行动空间，例如，在 $s_1$ 的状态下采取 $a_1$ 或 $a_4$ 会导致智能体与边界发生碰撞，因此我们可以设置 $s_1$ 的行动空间为 $\mathcal{A}(s_1)=\{a_{2},a_3,a_{5}\}$。在这本书中，我们考虑最一般的情况，即使其中有我们认为不合理的行动，我们也并不是人为去除，而是通过**算法来学习选择**。即，对任意状态 $s\in\mathcal{S}$，有 $\mathcal{A}=\{a_1,a_2,\ldots,a_5\}$。

![](./assets-of-强化学习-第一章-基本概念/2.png)

### 状态转移 (State Transition)

当执行一次行动时，智能体可能会从一个状态转移到另外一个状态。这样一个过程被称为**状态转移 (state transition)**。例如，如果智能体在状态 $s_1$ 处执行了行动 $a_2$(也就是向右移动)，那么此时智能体将会移动到状态 $s_2$，这样一个过程可以表示为:

$$s_1\xrightarrow{a_2}s_2$$

下面我们考虑两个重要且特殊的情况。

- **边界处理**：当智能体尝试跃出边界时，它下一步的状态应该是什么呢？例如，在 $s_1$ 时采取行动 $a_1$(也就是向上移动)时。因为智能体不可能跳出状态空间，所以智能体将会弹回到某一状态，在本书中设置为弹回原来的状态，这样一个状态转移过程表示为 $s_1\xrightarrow{a_1}s_1$。

- **禁区处理**：当智能体试图进入禁区时，它下一步的状态应该是什么呢？例如在 $s_5$ 时采取行动 $a_2$(也就是向右移动)。对某一状态处理不合法的动作空间，有下面两种不同的情况。
  - 在第一种情况下，**允许转移，但加以惩罚**，尽管 $s_6$ 是禁区，但是他仍然是可访问的，只不过进入的时候有惩罚。这种情况下，下一个状态就是 $s_6$。因此状态转移就是 $s_5\xrightarrow{a_2}s_6$。
  - 在第二种情况下，**不允许转移，被弹回到其他状态**， $s_6$ 是不可访问的，在这种情况下，智能体在尝试采取行动 $a_2$ 时将会弹回回到 $s_5$，因此这时候的状态转移过程就是 $s_5\xrightarrow{a_2}s_5$。

    我们究竟应该考虑哪种情况呢？在不同的场景下有不同的选择。在本书中，我们考虑第一种情况，即禁区是可以被访问的，只是智能体进入禁区会受到惩罚。这种情景更加一般化并且有趣。我们在后面的部分例子中可以看到智能体可能“冒险”穿过禁区，从而更快地到达目标区域。

每个状态的每一个行动都会对应一个状态转移过程。这个过程在表 1.1 中有所描述。在这个表中，每一行对应一个状态，每一列代表一个行动。每一个单元格给出了当智能体在对应的状态采取了对应行动后，将会转移到的下一个状态。

![](./assets-of-强化学习-第一章-基本概念/3.png)

> 表 1.1 状态过程的表格表示。每个单元格表示智能体在某一状态下采取行动后要过渡到的下一个状态。

在数学上，状态转移过程可以用条件概率来描述，例如，对于状态 $s_1$ 和行动 $a_2$ 的状态转移可以表示为：

$$\begin{cases}&p(s_1|s_1,a_2)=0,\\&p(s_2|s_1,a_2)=1,\\&p(s_3|s_1,a_2)=0,\\&p(s_4|s_1,a_2)=0,\\&p(s_5|s_1,a_2)=0,\end{cases}$$

该条件概率表明，在状态 $s_1$ 时采取行动 $a_2$ 时，智能体移动到状态 $s_2$ 的概率为 1，移动到其他状态的概率为 0。因此在状态 $s_1$ 处采取行动 $a_2$ 一定会导致智能体转移到 $s_2$。

表格表示法虽然直观，但只能描述**确定性**(deterministic)的状态转移过程。一般来说，状态转移也可能是**随机的**(stochastic)，此时需要用条件概率分布来描述。例如，当一阵随机的风吹过网格世界时，如果在状态 $s_1$ 处采取行动 $a_2$，智能体有可能会被吹到状态 $s_5$，而不是状态 $s_2$。在这种情况下，我们有 $p(s_5|s_1,a_2)>0$。

<TransitionProb />

不过简单起见，在本书网格世界的例子中，我们只考虑确定性的状态转移过程。

## 3. 策略 (Policy)

**策略**(policy)会告诉智能体**在每一个状态下应该采取什么行动**。直观上，策略可以被描述为图 1.4(a) 中的箭头。网格是一种可视化方法，在网格该如何解读呢？图片中的各个箭头就是在当前策略下，各个状态要采取什么动作了。如果智能体执行某一个策略，然后被放置到其中某个状态，那么它将会按照箭头从该状态开始，生成一条轨迹。

![](./assets-of-强化学习-第一章-基本概念/4.png)
> 图 1.4 一个确定性策略和对应的轨迹

在数学上，策略可以用条件概率来描述。将图 1.4 中的策略描述为 $\pi(a|s)$，表示在状态 $s$ 采取行动 $a$ 的概率。

上面例子中的策略是确定性的。但是策略也可能是随机的。例如，在图 1.5 给出了一种随机策略: 在状态 $s_1$ 下，智能体有 0.5 的概率采取向右移动，有 0.5 的概率采取向下移动，此时在状态 $s_1$ 的策略是：

$$\begin{gathered}\pi(a_1|s_1)=0,\\\pi(a_{2}|s_{1})=0.5,\\\pi(a_3|s_1)=0.5,\\\pi(a_4|s_1)=0,\\\pi(a_{5}|s_{1})=0,\end{gathered}$$

![](./assets-of-强化学习-第一章-基本概念/5.png)
> 图 1.5 一个随机策略。在状态 $s_1$ 中，智能体可能向右移动，也可能向下移动，概率均为 0.5。

除了用条件概率来表示策略，其也可以通过表格形式来描述。例如，表 1.2 展示了图 1.5 所示的随机策略。其中第 $i$ 行和第 $j$ 列的元素对应了在第 $i$ 个状态下采取第 $j$ 个行动的概率。这种表示法称为**表格表示法(tabular representation)，这种表示法也可以表示随机性策略，矩阵内元素为条件概率**。

![](./assets-of-强化学习-第一章-基本概念/6.png)
> 表 1.2 一个策略的表格表示法。

<PolicyDist />

## 4. 奖励 (Reward)

**奖励** (Reward)是强化学习中最重要的概念之一。**合适的奖励可以引导**智能体按照我们的**预期**来运动。为了设计出合适的奖励来实现我们的意图，**我们需要理解所给定的任务**。例如到达目标之后，如果也持续执行策略，需要合理设计奖励。

在一个状态下执行一个行动后，智能体会从环境反馈中获得奖励 $r$，其是状态 $s$和行动 $a$ 的函数，可以表示为 $r(s,a)$，其数值可以为正数、负数或零。

在所用的网格世界例子中，奖励可以设计如下:

- 如果智能体尝试跃出四周边界，则 $r_\text{boundary}=-1$
- 如果智能体尝试进入禁区，则 $r_\text{forbidden}=-1$
- 如果智能体到达目标区域，则 $r_\text{target}=+1$
- 其他情况下，智能体获得的奖励为 $r_\text{other}=0$

奖励的过程可以直观地表示为一个表格，如 1.3 所示。

![](./assets-of-强化学习-第一章-基本概念/7.png)
> 表 1.3: 奖励的表格表示法。

初学者可能会有这样一个问题：如果给定了奖励表格，我们是否能通过简单地选择奖励最大的行动来找到好的策略吗？答案是否定的。这是因为这些奖励都是**即时奖励** (immediate reward)，即在采取一个行动后立即获得的奖励。如果要确定一个好的政策，那么必须考虑更长远的**总奖励** (total reward)。具有最大即时奖励的行动不一定会带来最大的总奖励。

奖励的表格表示仅适用于确定性奖励，且一般仅为即时奖励表格。而我们还可以用条件概率来表示随机性的奖励 $p(r|s,a)$。获得奖励的过程有时候是随机的，受很多其他随机因素影响，所以是有随机性的。比如 $p(r = -1|s_1, a_2) = 0.5, p(r = -2|s_1, a_2) = 0.5$，即各有 0.5 的概率获得 -1 或者 -2 的奖励。值得强调的是，本书中的网格世界的例子都只考虑确定性的奖励过程


## 5. 轨迹、回报与回合 (Trajectory, Return, Episode)

### 轨迹 (Trajectory)

一条**轨迹** (trajectory)指的是一个**状态-行动-奖励**的链条。例如，从 $s_1$ 出发可能生成如下轨迹:

$$s_1 \xrightarrow[r=0]{a_2} s_2 \xrightarrow[r=0]{a_3} s_5 \xrightarrow[r=0]{a_3} s_8 \xrightarrow[r=1]{a_2} s_9.$$

沿着一条轨迹，智能体会得到一系列的即时奖励，这些即时奖励之和被称为**回报 (return)**。

### 回报 (Return)

**回报 (return)**是轨迹上所有奖励的总和，也可以被称为总奖励 (total reward) 或累积奖励 (cumulative reward)，由**即时奖励 (immediate reward)**和**未来奖励 (future reward)**组成。
* 即时奖励是在初始状态执行动作后立刻获得的奖励。
* 未来奖励指的是离开初始状态后获得的奖励之和。

回报可以用于评价一个策略的好坏。

轨迹也可以无限长，考虑一个无限长的轨迹，直接把这条轨迹上所有的奖励求和来计算回报，会发散到无穷大。为了防止无限循环导致的无穷大，且体现“即时奖励更重要”，我们引入**折扣因子 (Discount Factor) $\gamma$**：

$$\text{discounted return}= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

在这里 $\gamma \in (0,1)$ 被叫做**折扣因子** (discount rate)。用于允许考虑无限长的轨迹，用来调整对近期或远期奖励的重视程度。
- 当 $\gamma = 0$：智能体极其短视，只在乎眼前的利益。最后所得到的策略也会比较短视。
- 当 $\gamma \to 1$：智能体极其远视，认为未来的收益和现在一样重要。最后所得到的策略也会更具有远见。

折扣回报是所有折扣奖励的总和，即为不同时刻得到的奖励添加相应的折扣再求。


<ReturnCalc />

### 回合 (Episode)

当执行一个策略与环境进行交互时，智能体从初始状态开始到**终止状态 (terminal state)**停止的过程称为一个**回合 (episode)**或尝试 (trial)。请与神经网络训练过程中的回合 (epoch) 加以区分。

回合和轨迹在概念上非常相似，回合通常被认为是一条有限的长的轨迹。如果一个任务最多有有限步，那么这样的任务称为**回合制任务** (episodic task)。如果一个任务没有终止状态，则意味着智能体与环境的交互不会停止，这类任务称为**持续性任务** (continuing tasks)。

回合制任务转换为持续性任务有下面两种方法
- 把终止状态视为一个特殊状态，专门设计其动作空间或状态转移，从而使智能体永远停留在此状态，这样的状态被称为吸收状态。
- 将终止状态视为一个普通状态，设计折扣和奖励，让智能体学习到在到达这个状态之后能够保持原地不动。

## 6. 马尔可夫决策过程 (MDP)

本章前几节通过例子直观介绍了强化学习的一些基本概念。本节将在**马尔可夫决策过程** (MDP)的框架下，以更加正式的方式介绍这些概念。

马尔可夫决策过程涉及以下关键要素：

1. **集合**:
     - 状态空间 $\mathcal{S}$
     - 行动空间 $\mathcal{A}(s)$
     - 奖励集合 $\mathcal{R}(s,a)$
2. **模型**:
     - 状态转移概率 $p(s'|s,a)$
     - 奖励概率 $p(r|s,a)$
3. **策略**: $\pi(a|s)$
4. **马尔可夫性质**:
     $$p(s_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\ldots,s_0,a_0)=p(s_{t+1}|s_t,a_t)$$
     即下一个状态仅依赖于当前时刻的状态和行动。一旦在马尔可夫决策过程中的策略确定下来了，马尔可夫决策过程就退化成了一个马尔可夫过程。

![](./assets-of-强化学习-第一章-基本概念/9.png)
> 图 1.7: 将网格世界示例抽象为马尔可夫过程。在这里，圆圈代表状态，带箭头的链接代表状态转移。

## 7. 总结

本章介绍了强化学习中的基本概念。

| 概念 | 符号 | 物理含义 | 关键点 |
| :--- | :---: | :--- | :--- |
| **状态** | $s$ | 现在的处境 | 必须包含做决策所需的信息 |
| **动作** | $a$ | 能做什么 | 受限于当前状态 |
| **策略** | $\pi$ | 怎么做 | 这是一个从 $s$ 到 $a$ 的映射（或分布） |
| **奖励** | $r$ | 做的怎么样 | 引导行为的核心信号 |
| **回报** | $G_t$ | 最终能得多少分 | 引入折扣因子平衡当下与未来 |
