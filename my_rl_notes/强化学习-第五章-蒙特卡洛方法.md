# 强化学习-第五章-蒙特卡洛方法

此前我们介绍的算法（值迭代、策略迭代）都依赖于一个前提：我们知道环境的模型（Model-Based）。但在现实世界中，我们往往不知道概率 $p(s'|s,a)$ 是多少。

本章开始，我们进入 **Model-Free**（无模型）的世界。最先登场的是**蒙特卡洛方法 (Monte Carlo Methods)**。

> **核心思想**
> 不知道概率？没关系。扔无数次硬币，统计一下正面朝上的频率，那就是概率。
> **实践出真知**。

![](assets-of-强化学习-第五章-蒙特卡洛方法/chapter-5.png)

## 1. 蒙特卡洛估计：大数定律

蒙特卡洛方法的基础是**大数定律 (Law of Large Numbers)**。

如果我们要计算一个随机变量 $X$ 的期望 $\mathbb{E}[X]$，但不知道它的分布，怎么办？
我们可以从这个分布中采样 $N$ 次，得到 $x_1, x_2, \dots, x_N$，然后算平均值：
$$\mathbb{E}[X] \approx \frac{1}{N} \sum_{i=1}^{N} x_i$$
当 $N \to \infty$ 时，这个平均值会收敛到真实期望。

在强化学习中：
-   **目标**：估算 $q_\pi(s,a)$（即从 $s$ 出发选 $a$ 的期望回报）。
-   **做法**：让智能体玩很多轮游戏（Episodes）。统计所有经过 $(s,a)$ 的轨迹，计算它们的实际回报 $G_t$，然后取平均。

::: info 交互演示：大数定律 (Law of Large Numbers)
(此处预留交互式组件：抛硬币或掷骰子模拟器。随着次数增加，实际频率曲线逐渐逼近理论概率直线)
:::

## 2. 最基础的 MC 算法 (MC Basic)

既然能估算 $q(s,a)$，那我们就可以用类似“策略迭代”的路子：
1.  **策略评估**：用蒙特卡洛方法估算当前策略 $\pi$ 的 $q_\pi(s,a)$。
2.  **策略提升**：对每个状态，选 $q$ 值最大的动作作为新策略。

### 遇到的问题
这就好比：“我只走我熟悉的路去学校，并且算出了这条路平均要 20 分钟。但我从来不走小巷子，所以我永远不知道小巷子是不是更快。”

如果策略是**确定性**的（Deterministic），我们可能永远无法访问某些 $(s,a)$ 对。这就叫**探索性不足**。

## 3. MC Exploring Starts

为了解决探索性问题，一种简单粗暴的方法是：**Exploring Starts (探索性出发)**。

**假设**：我们有上帝视角，可以把智能体直接“瞬移”到任意状态 $s$，并强迫它执行任意动作 $a$ 作为开局。
这样就能保证所有 $(s,a)$ 都有机会被访问到。

但现实中这很难实现（你没法把自动驾驶汽车瞬移到高速公路上并在逆行状态下开局）。

![](assets-of-强化学习-第五章-蒙特卡洛方法/11.png)

## 4. MC $\varepsilon$-Greedy

更实用的方法是：**不要让策略变确定**。我们允许策略犯错，或者说“故意乱走”。

这就是 **$\varepsilon$-Greedy 策略**：
-   以 $1 - \varepsilon$ 的概率：**贪婪** (Exploit)，选目前认为最好的动作。
-   以 $\varepsilon$ 的概率：**探索** (Explore)，随机选一个动作。

$$\pi(a|s) = \begin{cases} 1 - \varepsilon + \frac{\varepsilon}{|\mathcal{A}|} & \text{if } a = a^* \\ \frac{\varepsilon}{|\mathcal{A}|} & \text{if } a \neq a^* \end{cases}$$

这样，只要 $\varepsilon > 0$，所有动作都有机会被选中。我们就不需要“瞬移”了。

::: info 交互演示：Epsilon-Greedy 选择器
(此处预留交互式组件：一个饼图展示动作选择概率。调节滑块 $\varepsilon$，看到“最佳动作”的份额和“随机噪声”的份额如何变化)
:::

## 5. 探索与利用的权衡 (Exploration vs Exploitation)

-   $\varepsilon$ 太大：学得快，见过世面，但因为老是乱走，平均分上不去（最优策略的表现受损）。
-   $\varepsilon$ 太小：收敛到最优策略后的表现好，但可能要在局部最优解里困很久才能发现新大陆。

通常的做法是：**Decay Epsilon**。开始时 $\varepsilon$ 大一点（多探索），随着时间推移，降低 $\varepsilon$（多利用，趋向于收敛）。

::: info 交互演示：探索衰减 (Exploration Decay)
(此处预留交互式组件：折线图展示 $\varepsilon$ 随时间衰减的曲线，以及对应的“平均回报”预期变化)
:::

## 6. 总结

蒙特卡洛方法是强化学习迈向“无模型”的第一步。它简单、无偏（Unbiased），但也有缺点：
-   **需要完结**：必须等一局游戏彻底结束才能算分 (Episode-based)。
-   **方差大**：一局游戏里的随机因素太多，导致回报 $G_t$ 波动很大，收敛慢。

| 方法 | 核心机制 | 优点 | 缺点 |
| :--- | :--- | :--- | :--- |
| **MC Basic** | 简单的平均 | 直观 | 需要 Exploring Starts |
| **MC $\varepsilon$-Greedy** | 软策略 (Soft Policy) | 保证探索 | 策略总是不完美的 (含 $\varepsilon$ 噪声) |

下一章，我们将介绍 **时序差分 (TD) 学习**，结合动态规划（自举）和蒙特卡洛（采样）的优点，解决 MC “必须等游戏结束”的痛点。
