# 强化学习-第七章-时序差分学习

本章介绍强化学习中最重要、最著名的一类算法：**时序差分 (Temporal Difference, TD) 学习**。

它结合了前两章的优点：
1.  像**蒙特卡洛 (MC)**：不需要环境模型，直接从经验中学习（Sampling）。
2.  像**动态规划 (DP)**：不需要等一局结束，利用当前的估计来更新（Bootstrapping）。

![](assets-of-强化学习-第七章-时序差分学习/chapter-7.png)

## 1. TD 策略评价：TD(0)

回顾一下 Robbins-Monro 算法在 RL 中的应用。我们需要求解贝尔曼方程 $v_\pi(s) = \mathbb{E}[R + \gamma v_\pi(S')]$。

-   **蒙特卡洛 (MC)** 的做法：
    $$v(S_t) \leftarrow v(S_t) + \alpha [\underbrace{G_t}_{\text{Target}} - v(S_t)]$$
    Target 是 $G_t$（实际跑完一局的总回报）。必须等游戏结束才能算。

-   **时序差分 (TD)** 的做法：
    $$v(S_t) \leftarrow v(S_t) + \alpha [\underbrace{R_{t+1} + \gamma v(S_{t+1})}_{\text{Target}} - v(S_t)]$$
    Target 是 $R_{t+1} + \gamma v(S_{t+1})$（走一步，拿到奖励，加上对下一步的估计）。**不用等游戏结束！**

这种方法叫 **TD(0)**（一步 TD）。括号里的差值 $\delta_t = R_{t+1} + \gamma v(S_{t+1}) - v(S_t)$ 被称为 **TD Error**。

::: info 交互演示：TD vs MC 更新时机
(此处预留交互式组件：一条时间轴。MC 在 Episode 结束时回溯更新所有状态；TD 在每一步 $t$ 就立即更新 $S_t$。直观展示 TD 的“即时性”。)
:::

## 2. Sarsa: On-Policy Control

既然能估算 $v$，就能估算 $q$。将 TD 原理应用到动作价值上，就得到了 **Sarsa** 算法。

公式：
$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$$

仔细看这个公式，它需要用到 $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$ 这五个元素，所以得名 **Sarsa**。

### 算法流程
1.  在 $S$ 选 $A$ (用 $\varepsilon$-greedy)。
2.  执行 $A$，得到 $R$ 和 $S'$。
3.  在 $S'$ 选 $A'$ (用 $\varepsilon$-greedy，因为是 On-Policy，下一步真的会执行 $A'$)。
4.  利用 $(S, A, R, S', A')$ 更新 $Q(S, A)$。
5.  $S \leftarrow S', A \leftarrow A'$。

::: info 交互演示：Sarsa 一步详解
(此处预留交互式组件：展示 $(S, A, R, S', A')$ 五元组的高亮过程。强调 $A'$ 是真的被“选出来准备执行”的。)
:::

## 3. Q-Learning: Off-Policy Control

Sarsa 是 On-Policy 的，意味着它学习的 value 是“虽然我尽力想贪婪，但我不得不偶尔乱走 ($\varepsilon$)”的价值。这导致它比较胆小。

**Q-Learning** 提出了一个大胆的想法：
-   虽然我行动上必须乱走 (Behavior Policy: $\varepsilon$-greedy) 以保持探索...
-   但我心里估算价值时，通过**假设**下一步我能做得最好 (Target Policy: Greedy) 来估算！

公式：
$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a) - Q(S_t, A_t)]$$

注意 Target 变成了 $\max_a Q(S_{t+1}, a)$。不管实际上 $A_{t+1}$ 选了什么，我都用最好的那个来更新。这就是 **Off-Policy**。

### 经典案例：悬崖行走 (Cliff Walking)
-   **Sarsa**: 既然有 $\varepsilon$ 概率掉坑里，那我干脆离悬崖远点，走安全路线（虽然远）。
-   **Q-Learning**: 心里想的是“如果我不犯错，走悬崖边最近”，所以学出的是最优路线（悬崖边）。但实际跑的时候，因为 $\varepsilon$ 的存在，偶尔会掉下去。

![](assets-of-强化学习-第七章-时序差分学习/7.png)

::: info 交互演示：悬崖行走对比
(此处预留交互式组件：重现悬崖行走环境。用户切换 Sarsa/Q-Learning，可以直接看到 Sarsa 贴着墙根走，Q-Learning 贴着悬崖走。)
:::

## 4. n-step Sarsa

TD(0) 只看一步，MC 看无穷步。中间能不能折中？
可以！我们可以看 $n$ 步：
$$G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1} R_{t+n} + \gamma^n Q(S_{t+n}, A_{t+n})$$

这也是一种权衡：
-   $n$ 小：偏差大（依赖估计），方差小。
-   $n$ 大：偏差小（依赖真实奖励），方差大。

::: info 交互演示：n-Step 视野 (n-Step Horizon)
(此处预留交互式组件：一个滑块调节 $n$。$n=1$ 时显示仅关联下一步，$n=\infty$ 时显示关联整个轨迹。)
:::

## 5. 总结：统一视角

我们发现，这些算法其实都是同一个大框架下的变体。

| 算法 | Target 计算 | 策略类型 | 特点 |
| :--- | :--- | :--- | :--- |
| **TD(0) / Sarsa** | $R + \gamma Q(S', A')$ | On-Policy | 稳健，学习的是“实际表现” |
| **Q-Learning** | $R + \gamma \max Q(S', a)$ | Off-Policy | 激进，学习的是“最优表现” |
| **Monte Carlo** | $G_t$ | On-Policy | 准确但方差大，需要回合结束 |

TD Learning 是现代强化学习的基石。基于 Q-Learning 的 DQN 算法更是开启了深度强化学习的时代。下一章，我们将稍微休息一下数学推导，来看看**值函数近似 (Value Function Approximation)**，这将带我们走向深度学习的怀抱。
