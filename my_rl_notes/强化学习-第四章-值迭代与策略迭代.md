# 强化学习-第四章-值迭代与策略迭代

本章的核心是解决上一章提出的**贝尔曼最优方程**。我们将介绍两种最基本的**动态规划 (Dynamic Programming)** 算法：**值迭代 (Value Iteration)** 和 **策略迭代 (Policy Iteration)**。

> **前提条件**
> 这些算法都假设我们拥有环境的完美模型（即知道所有 $p(s'|s,a)$ 和 $p(r|s,a)$）。这被称为 **Model-Based** RL。

![](assets-of-强化学习-第四章-值迭代与策略迭代/chapter-4.png)

## 1. 值迭代 (Value Iteration)

值迭代可以看作是“简单粗暴”地通过反复迭代来逼近贝尔曼最优方程的解。

### 算法步骤
既然贝尔曼最优方程是：
$$v^*(s) = \max_a (r_{\pi} + \gamma P_{\pi} v^*)$$
那我们就把它变成一个迭代公式：

$$v_{k+1}(s) = \max_{a \in \mathcal{A}} \left[ \sum_{r} p(r|s, a) r + \gamma \sum_{s'} p(s'|s, a) v_{k}(s') \right]$$

这实际上包含了两步（隐式地）：
1.  **策略更新**：在当前的想法 ($v_k$) 下，哪个动作看起来最好？
2.  **值更新**：把那个最好的动作带来的价值作为新的估计值 ($v_{k+1}$)。

虽然听起来有点循环论证（用此时的估计去更新下一刻的估计），但数学保证了它一定会收敛到最优解 $v^*$。

::: info 交互演示：值迭代步骤 (Value Iteration Step-by-Step)
(此处预留交互式组件：展示一个 3x3 网格，每个格子有一个值。用户点击“迭代一步”，看到每个格子如何扫描周围邻居并取最大值来更新自己)
:::

### 例子
在 GridWorld 中，如果我们运行几次值迭代：
1.  $k=1$：除了目标点旁边的一圈，其他都是 0。
2.  $k=2$：价值开始向外扩散。
3.  $k \to \infty$：所有位置都找到了通往目标的最优路径的价值。

![](assets-of-强化学习-第四章-值迭代与策略迭代/2.png)

## 2. 策略迭代 (Policy Iteration)

策略迭代将“找最优策略”这个过程显式地分成了两步循环。

### 算法循环
1.  **策略评估 (Policy Evaluation, PE)**：
    -   给定当前策略 $\pi_k$，算出它到底有多好（求出 $v_{\pi_k}$）。
    -   这通常需要解一个线性方程组（或用几次迭代估算）。
2.  **策略提升 (Policy Improvement, PI)**：
    -   有了 $v_{\pi_k}$，我们在每个状态都选那个“看起来最好”的动作（贪心）。
    -   $\pi_{k+1}(s) = \arg \max_a q_{\pi_k}(s, a)$

如此循环，直到策略不再变化。

::: info 交互演示：策略迭代循环 (Policy Iteration Loop)
(此处预留交互式组件：演示两个视图。左边是“策略评估”，颜色逐渐填满；右边是“策略提升”，箭头方向改变。周而复始)
:::

### 例子
看下图，随着迭代进行，策略从最初的随机乱走（图 a），一步步修正（图 b, c），最终变成了最优策略（图 h）。注意：**越靠近目标的地方，策略越先优化好**（近水楼台先得月）。

![](assets-of-强化学习-第四章-值迭代与策略迭代/8.png)

## 3. 截断策略迭代 (Truncated Policy Iteration)

这两者有什么关系？

- **策略迭代**：在“策略评估”这一步，非要算出**精确**的 $v_{\pi_k}$ 才肯罢休（可能需要无限次迭代或解方程）。
- **值迭代**：在“策略评估”这一步，只算**一步**就急着去更新策略了。

其实，我们没必要非得算出精确的 $v_{\pi_k}$。算个大概，方向对了，就可以去修策略了。这就叫**截断策略迭代**。

$$
\text{值迭代 (1 步)} \subset \text{截断策略迭代 (n 步)} \subset \text{策略迭代 (}\infty \text{ 步)}
$$

::: info 交互演示：截断步数的影响 (Truncation Slider)
(此处预留交互式组件：一个滑块调节“策略评估步数”。步数少=收敛慢但每轮快；步数多=每轮由于算得准所以策略提升显著，但每轮慢)
:::

## 4. 总结

本章介绍的算法是强化学习的基石，虽然它们需要模型（Model-Based），但其思想（**评估-提升**循环）贯穿了整个 RL 领域。

| 算法 | 核心思想 | 优点 | 缺点 |
| :--- | :--- | :--- | :--- |
| **值迭代** | 直接迭代最优方程 | 每步计算简单 | 这个“每步”可能收敛得很慢 |
| **策略迭代** | 显式分离评估与提升 | 收敛需要的轮数少 | 每轮里面的“评估”很费时间 |
| **截断策略迭代** | 折中方案 | 平衡了计算量和收敛速度 | 需要调参（截断步数） |

下一章，我们将扔掉“环境模型”这个拐杖，学习**蒙特卡洛方法**，在未知的世界中通过试错来学习。
