# 强化学习-第三章-贝尔曼最优方程

本章与前后两章关系密切：第 2 章中我们介绍了贝尔曼方程，本章将要介绍的贝尔曼最优方程是一个 **特殊的** 贝尔曼方程；第 4 章将介绍的“值迭代”算法就用于求解本章介绍的贝尔曼最优方程。因此，本章起到了承上启下的关键作用。

核心概念：**最优策略 (Optimal Policy)** 和 **贝尔曼最优方程 (Bellman Optimality Equation)**。

> **我们为什么要学习这个？**
> 前两章我们学会了如何**评价**一个给定的策略（算分）。这一章我们要学会如何**找到**最好的策略（赢）。

![](assets-of-强化学习-第三章-贝尔曼最优方程/chapter-3.png)


## 1. 最优策略：直观理解

我们先看一个简单的例子。回顾第二章的例子，我们计算出了某个策略下状态 $s_1$ 的状态值 $v_\pi(s_1) = 6.2$，以及从 $s_1$ 出发采取各个行动的**行动值** (Action Value)：

$$\begin{aligned}
&q_{\pi}(s_1, a_1) = -1 + \gamma v_{\pi}(s_1) = 4.58 \\
&q_{\pi}(s_1, a_2) = -1 + \gamma v_{\pi}(s_2) = 8 \\
&q_{\pi}(s_1, a_3) = 0 + \gamma v_{\pi}(s_3) = 9 \\
&q_{\pi}(s_1, a_4) = -1 + \gamma v_{\pi}(s_1) = 4.58 \\
&q_{\pi}(s_1, a_5) = 0 + \gamma v_{\pi}(s_1) = 5.58
\end{aligned}$$
*(注：假设 $\gamma=0.9, v(s_1)=6.2, v(s_2)=10, v(s_3)=10$ 等)*

显然，$q_{\pi}(s_1, a_3) = 9$ 是最大的。
这意味着：在这个状态，虽然原来的策略 $\pi$ 让我们有一定的概率乱走，但**如果我只选 $a_3$**，我能得到的期望回报更高！

这个例子说明了：**如果我们修改策略，使之选择具有最大行动值的行动，就可以获得更好的策略。**

::: info 交互演示：策略提升 (Policy Improvement)
(此处预留交互式组件：用户点击“计算 q 值”，组件高亮显示最大值 $a_3$，并将该状态的策略箭头更新为指向 $a_3$)
:::

## 2. 最优策略：定义

如果一个策略 $\pi^*$ 比所有其他可能的策略 $\pi$ 都要好（或至少一样好），那么它就是**最优策略**。

**定义 3.1 (最优策略)**：
策略 $\pi^*$ 是最优的，当且仅当对于任意状态 $s \in \mathcal{S}$ 和任意其他策略 $\pi$，都有：
$$v_{\pi^*}(s) \geq v_\pi(s)$$

这个时候，$\pi^*$ 对应的状态值即使**最优状态值**，记为 $v^*(s)$。

这个定义引出了三个灵魂拷问：
1.  **存在性**：这样的最优策略一定存在吗？（会不会出现剪刀石头布的情况，A>B, B>C, C>A？）
2.  **唯一性**：最优策略只有一个吗？
3.  **算法**：怎么找到它？

答案是：**一定存在**，**不一定唯一（未必只有一个赢法）**，**有算法能找到**。

## 3. 贝尔曼最优方程 (Bellman Optimality Equation)

对于普通策略，且 $v_\pi(s)$ 满足贝尔曼方程（线性的）：
$$v_{\pi}(s) = \sum_{a} \pi(a|s) q_{\pi}(s,a)$$
即状态值是行动值的加权平均。

对于**最优策略** $\pi^*$，它一定只选那个**最好**的动作。所以最优状态值 $v^*(s)$ 必定等于那个最大的行动值：
$$v^*(s) = \max_{a \in \mathcal{A}} q^*(s, a)$$

展开 $q^*(s,a)$，我们得到了**贝尔曼最优方程**：

$$v^*(s) = \max_{a \in \mathcal{A}} \underbrace{\sum_{r} p(r|s, a) r + \gamma \sum_{s'} p(s'|s, a) v^*(s')}_{q^*(s, a)}$$

### 关键区别
- **贝尔曼方程**：$v_\pi = r_\pi + \gamma P_\pi v_\pi$ （线性的，好解）
- **贝尔曼最优方程**：$v^* = \max_a (r_a + \gamma P_a v^*)$ （**非线性**的，因为有个 $\max$ 操作，难解）

数学上有一个很强的定理（**压缩映射定理**）保证了这个方程一定有解，而且是唯一的 $v^*$。

## 4. 求解最优策略

虽然方程是非线性的，不能像第二章那样直接求逆矩阵解，但我们可以用迭代的方法去逼近它。

### 确定性最优策略定理
**定理**：对于任意有限 MDP，总是存在一个**确定性**的策略 $\pi^*$，它是最优的。
即：我们不需要搞复杂的随机策略（比如 30% 向左，70% 向右），只需要简单粗暴地：**在这个状态，就往分最高的地方走**。

$$\pi^*(a|s) = \begin{cases} 1 & \text{if } a = \arg \max_a q^*(s, a) \\ 0 & \text{otherwise} \end{cases}$$

::: info 交互演示：值迭代与 Max 算子
(此处预留交互式组件：展示一个简单的 2x2 网格，用户点击“迭代一步”，看到每个格子的值如何通过取邻居的 max 值更新自己)
:::

## 5. 示例与折扣因子的作用

看看下面的例子，有两种策略：
1.  **绕路策略**：为了安全，绕一大圈去目标。
2.  **最优策略**：直接穿过狭窄的通道（假设没有惩罚风险），或者在有风险时权衡利弊。

![](assets-of-强化学习-第三章-贝尔曼最优方程/5.png)

这里**折扣因子 $\gamma$** 起到了关键作用：
- 如果 $\gamma \approx 0$（短视），智能体只看眼前。如果眼前全是 0 分，它可能就懒得动，或者随便走。
- 如果 $\gamma \approx 1$（远视），智能体会为了远处的 +100 分而忍受沿途的长途跋涉。

在图 3.5 的例子中，如果绕路太远，$\gamma^{10} \times 1$ 可能还不如直接撞墙扣分 $(-1) + \gamma \times 1$ 划算（假设撞墙能穿过去）。**$\gamma$ 实际上定义了智能体的“耐心”**。

::: info 交互演示：Gamma 与路径规划
(此处预留交互式组件：一个滑块调节 Gamma 值。低 Gamma 时，Agent 选择最近的一个小奖励；高 Gamma 时，Agent 忽略近处小奖励，直奔远处大奖励)
:::

## 6. 总结

本章我们升级了装备：从**评价者**变成了**决策者**。

| 概念 | 对应方程 | 数学特性 | 目的 |
| :--- | :--- | :--- | :--- |
| **策略评价** | 贝尔曼方程 | 线性方程组 <br> $v = (I - \gamma P)^{-1} r$ | 算分：这个策略怎么样？ |
| **策略优化** | **贝尔曼最优方程** | 非线性方程 (含 $\max$) <br> 无解析解，需迭代 | **求解：怎么做才最好？** |

下一章，我们将正式学习如何用算法（值迭代）来解开这个最优方程。
