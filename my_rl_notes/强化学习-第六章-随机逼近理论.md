# 强化学习-第六章-随机逼近理论

这一章可能看起来有点“离题”，因为它讲的是纯数学：**随机逼近 (Stochastic Approximation)**。

但请不要跳过！这是强化学习（特别是 Model-Free 部分）的**引擎**。
你是否好奇过：为什么公式里总是有个 $\alpha$ (学习率)？为什么它的更新公式张这样：$New \leftarrow Old + \alpha (Target - Old)$？
答案就在本章。

![](assets-of-强化学习-第六章-随机逼近理论/chapter-6.png)

## 1. 启发式：如何估计期望？

回顾上一章的蒙特卡洛，我们用平均值来估算期望：
$$w_{k+1} = \frac{1}{k} \sum_{i=1}^{k} x_i$$
这个式子每次都要把所有数加起来，很麻烦。能不能变成**增量式 (Incremental)** 的？

$$
\begin{aligned}
w_{k+1} &= \frac{1}{k+1} \sum_{i=1}^{k+1} x_i \\
&= \frac{1}{k+1} (x_{k+1} + \sum_{i=1}^k x_i) \\
&= \frac{1}{k+1} (x_{k+1} + k \cdot w_k) \\
&= w_k - \frac{1}{k+1} (w_k - x_{k+1})
\end{aligned}
$$

我们得到一个通用的更新结构：
$$w_{New} = w_{Old} + \alpha_k \cdot (x - w_{Old})$$
其中 $x - w_{Old}$ 是**误差 (Error)**，$\alpha_k = \frac{1}{k+1}$ 是**步长 (Step Size)**。

::: info 交互演示：动态步长 (Dynamic Step Size)
(此处预留交互式组件：对比两条曲线。一条使用 $\alpha=1/k$，迅速收敛；另一条使用 $\alpha=0.1$（常数），虽然快但也永远在真实值附近震荡)
:::

## 2. Robbins-Monro 算法

更一般地，假设我们想求一个方程 $g(w)=0$ 的根 $w^*$。
但是，我们**不知道** $g(w)$ 的具体表达式，我们只能通过“观测”得到一个带有噪声的值 $\tilde{g}(w, \eta)$。

**Robbins-Monro (RM) 算法** 告诉我们，只要按这个公式迭代：
$$w_{k+1} = w_k - \alpha_k \tilde{g}(w_k, \eta_k)$$
就能找到那个根 $w^*$！

### 步长条件
只要步长序列满足两个条件：
1.  **足够大**：$\sum_{k=1}^{\infty} \alpha_k = \infty$ （能走到任意远的地方，防止半路停下）
2.  **足够小**：$\sum_{k=1}^{\infty} \alpha_k^2 < \infty$ （随着时间推移，步子收得足够快，防止在终点震荡）

经典的选择是 $\alpha_k = 1/k$。但在 RL 实践中，为了适应环境的变化（非平稳），我们常使用小的常数 $\alpha$。

::: info 交互演示：求根过程 (Root Finding)
(此处预留交互式组件：这是整个 RL 算法的抽象。用户只要给出一个带有噪声的函数，RM 算法就能像神射手一样慢慢逼近零点)
:::

## 3. Dvoretzky 定理与 RL 的联系

Robbins-Monro 算法看起来是解方程，和 RL 有什么关系？

实际上，**贝尔曼方程** $v_\pi(s) = \mathbb{E}[R + \gamma v_\pi(S')]$ 可以写成：
$$\underbrace{v_\pi(s) - \mathbb{E}[R + \gamma v_\pi(S')]}_{g(v)} = 0$$

这不就是求 $g(v)=0$ 的根吗？
-   我们观测到的“噪声样本”是：$R + \gamma v(S')$ (TD Target)。
-   我们的更新公式是：$v(S) \leftarrow v(S) + \alpha (Target - v(S))$。

这**完全**就是 Robbins-Monro 算法！

## 4. 总结

本章虽然短，但解释了 Model-Free RL 的数学本质。

-   **大数定律** $\to$ **蒙特卡洛均值** $\to$ **Robbins-Monro 迭代**。
-   下章的 **TD 学习**，本质上就是用 RM 算法来解贝尔曼方程。

| 概念 | 数学形式 | 在 RL 中的对应 |
| :--- | :--- | :--- |
| **求期望** | $\mathbb{E}[X]$ | 估算价值函数 $v(s)$ |
| **观测样本** | $x_k$ | 回报 $G_t$ 或 $R+\gamma v$ |
| **RM 算法** | $w \leftarrow w + \alpha (x - w)$ | TD 更新 / Q-Learning 更新 |

准备好了吗？下一章我们将把这个强大的数学工具应用到贝尔曼方程上，诞生出强化学习最著名的算法：**TD Learning**。
