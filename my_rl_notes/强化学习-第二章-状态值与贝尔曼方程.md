# 强化学习-第二章-状态值与贝尔曼方程

第二章将介绍一个核心概念和一个核心工具。核心概念是状态值，它是一个评价策略好坏的重要指标。既然状态值这么重要，那么我们应该如何分析它呢？答案就是核心工具**贝尔曼方程**(Bellman equation)。贝尔曼方程描述了所有状态值之间的关系。通过求解贝尔曼方程，我们就可以得到状态值，进而评价一个策略的好坏。

![](assets-of-强化学习-第二章-状态值与贝尔曼方程/chapter-2.png)

## 1. 状态值 (State Value)

### 回报的计算方法

上一章介绍了回报的概念，它在强化学习中扮演着重要的角色，因为它可以评估一个策略的好坏。

通过下图的例子，不同策略在 $s_1$ 的选择导致了不同的轨迹。直观上，能避开禁区的策略显然更好。数学上，通过计算回报（Return），我们可以量化这种“好坏”。

- **策略 1 (左)**: 避开禁区，回报最大。
- **策略 2 (中)**: 进入禁区，回报最小。
- **策略 3 (右)**: 50% 概率进入禁区，回报居中。

这证明了**回报可以被用来评估策略的好坏**。

![](assets-of-强化学习-第二章-状态值与贝尔曼方程/1.png)

计算回报有两种方法：

- **定义法**：回报等于沿轨迹收集的所有奖励的折扣总和。
  $$v_{1}=r_1+\gamma r_2+\gamma^2r_3+\ldots$$

- **自举法 (Bootstrapping)**：由定义推导，可以用自举重写。
  $$v_{1}=r_1+\gamma(r_2+\gamma r_3+\ldots)=r_1+\gamma v_2$$

这个自举法揭示了下面一些要点：
- 从不同状态出发的回报值是彼此依赖的
- 从一个状态出发的回报值依赖于从其他状态出发获得的回报。
- 还有**贝尔曼方程的雏形**，即未知的价值也是自身的函数。

![](assets-of-强化学习-第二章-状态值与贝尔曼方程/2.png)

### 状态值引入的必要性

回报并不适用于一般化的随机情况，因为从一个状态出发可能会得到不同的轨迹和回报。为此，我们引入**状态值**。

**状态值函数** $v_\pi(s)$ 定义为在状态 $s$ 下，遵循策略 $\pi$ 时的预期回报：

$$v_\pi(s) = \mathbb{E}_\pi[G_t|S_t=s]$$

>  **因为 $\displaystyle S_t \xrightarrow{A_t} S_{t+1}, R_{t+1} \xrightarrow{A_{t+1}} S_{t+2}, R_{t+2} \xrightarrow{A_{t+2}} S_{t+3}, R_{t+3}, \dots$ 中的每一步的状态和即时回报都是随机的，所以 $\displaystyle G_t$ 也是随机的**
> - **随机性来源**: $S_t \xrightarrow{A_t} S_{t+1}, R_{t+1} \dots$ 每一步的状态和即时回报都是随机的，所以 $G_t$ 也是随机变量。
> - **期望的意义**: 因为 $G_t$ 是随机的，我们无法用它直接评估，所以取其**期望** $v_\pi(s)$。
> - **依赖关系**:
> 	- 依赖于 $s$，即不同状态的状态值不同，这是因为条件期望 $\mathbb{E}[G_t|S_t=s]$ 依赖于 $s$。
> 	- 依赖于 $\pi$，不同策略，轨迹可能不同
> 	- **不**依赖于 $t$，什么时候 $\displaystyle t$ 到让 $\displaystyle S_{t}$ 到某个状态 $\displaystyle s$，是不重要的

与回报的关系如下
- 当策略和系统模型都是确定性时，从一个状态出发始终会得到相同的轨迹。此时，从一个状态出发得到的回报就等于状态值。
- 当策略或系统模型是随机的时，从一个状态出发可能产生不同的轨迹。此时，不同轨迹的回报是不同的,而状态值就是这些回报的期望值。

## 2. 贝尔曼方程 (Bellman Equation)

状态值函数 $v_\pi(s)$ 可以用自举法重写，得到的方程称为贝尔曼方程：

$$
\begin{aligned}
v_{\pi}(s) &= \mathbb{E}[G_t | S_t = s] \\
&= \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
&= \mathbb{E}[R_{t+1} | S_t = s] + \gamma \mathbb{E}[G_{t+1} | S_t = s] \\
&= \underbrace{\sum_{a \in \mathcal{A}} \pi(a|s) \sum_{r \in \mathcal{R}} p(r|s, a)r}_{\text{即时奖励的期望}} + \underbrace{\gamma \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{s' \in \mathcal{S}} p(s'|s, a)v_{\pi}(s')}_{\text{未来奖励的期望}} \\
&= \sum_{a \in \mathcal{A}} \pi(a|s) \left[ \sum_{r \in \mathcal{R}} p(r|s, a)r + \gamma \sum_{s' \in \mathcal{S}} p(s'|s, a)v_{\pi}(s') \right]
\end{aligned}
$$

### 理解方程中的随机性分解

这个方程极其精妙地把**两层随机性**解耦了：

1.  **策略的随机性** ($\pi(a|s)$): 外面的求和，是该策略在该状态下所有可能动作的加权平均值。
	- 	给定策略，在其某一状态可以做的动作可以是随机的，这是**对策略随机性的建模**。
	-	所以需要求和 $\displaystyle \sum_{a \in \mathcal{A}}\pi (a | s)$ 来计算该状态下所有可能动作的加权平均值。
    
2.  **环境(模型)奖励的随机性** ($p(r|s, a)$ 和 $p(s'|s, a)$): 中括号里面的，是采取某个具体动作的情况下的即时奖励和未来奖励的期望值.
    -   中括号里是对“采取某个具体动作 $a$”后的后果评估。
	-	对于当前状态，其即时奖励 $\displaystyle r$ 就是衡量当前状态的好坏的价值，和另外一边的 $\displaystyle v_{\pi}(s')$ 对应。
	- 	并且两边都有 $\displaystyle s,a$ 的条件概率去兑换出价值，这是**对环境奖励的随机性的建模**。
    -   $\sum p(r|s, a)r$ 是动作 $a$ 带来的**即时奖励**期望。
    -   $\sum p(s'|s, a)v_{\pi}(s')$ 是动作 $a$ 带来的**未来价值**期望。


$\displaystyle v_{\pi}(s)$ 和 $\displaystyle v_{\pi}(s')$ 是需要计算的状态值,是未知量。如果单独看这个方程,因为状态值存在于等式的两边,读者可能难以理解如何求解。必须要注意的是,每一个状态 s∈S都对应了一个贝尔曼方程,所以我们有一组这样的式子。如果我们将这些式子联立, 如何计算所有状态值就变得很清晰了

$\displaystyle \pi(a\mid s)$ 是一个给定的策略,是已知量。贝尔曼方程一定是对应了一个特定的策略。但是一个给定的策略，求解贝尔曼方程从而得到状态值是一个策略评价(policy evaluation)的过程,这是许多强化学习算法中的核心步骤。

$\displaystyle p (r |  s, a)$ 和 $\displaystyle p(s'|s, a)$ 代表系统模型,这是已知量还是未知量呢? 现在我们将首先研究模型已知的情况。在后面的章节中,我们会慢慢推广到不需要模型的情况。

### 三种等价形式

为了方便推导，贝尔曼方程常写成以下形式：

1.  **贝尔曼期望方程** (利用期望算子): 
	- 第一种形式，利用 $\displaystyle G_{t+1}=\mathbb{E}[v_{\pi}(S_{t+1})|S_t=s]$，带入 $\displaystyle v_{\pi}(s)=\mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_t=s]$ 中
	- $\displaystyle v_{\pi}(s)=\mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_t=s]$
2.  **全概率展开形式** (最常用):
	- 用全概率公式展开 $\left\{ \begin{aligned} p(s'|s, a) &= \sum_{r \in \mathcal{R}} p(s', r|s, a), \\ p(r|s, a) &= \sum_{s' \in \mathcal{S}} p(s', r|s, a). \end{aligned} \right.$
	- $\displaystyle v_{\pi}(s)=\sum_{a\in \mathcal{A}} \pi(a|s) \sum_{s'\in \mathcal{S}} \sum_{r\in \mathcal{R}} p(s',r|s,a) [r + \gamma v_{\pi}(s')]$
	- $\displaystyle r$ 在某些问题中可能仅依赖于下一个状态,即奖励可以写成 $\displaystyle r(s')$
3.  **矩阵-向量形式的简记**:
	- **第三种形式是记号的简记（为了推导矩阵-向量形式）**
	- 方程重写为：
		$\displaystyle v_{\pi}(s) = r_{\pi}(s) + \gamma \sum_{s' \in \mathcal{S}} p_{\pi}(s'|s)v_{\pi}(s')$
	- 其中，即时奖励的期望定义为：
		$\displaystyle r_{\pi}(s) \doteq \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{r \in \mathcal{R}} p(r|s, a)r$ 
	- 状态转移概率定义为：
		$\displaystyle p_{\pi}(s'|s) \doteq \sum_{a \in \mathcal{A}} \pi(a|s)p(s'|s, a)$
	- 这种形式可以直接写成线性代数的矩阵形式 $\mathbf{v}_{\pi} = \mathbf{r}_{\pi} + \gamma \mathbf{P}_{\pi} \mathbf{v}_{\pi}$，从而直接求解 $\mathbf{v}_{\pi} = (\mathbf{I} - \gamma \mathbf{P}_{\pi})^{-1}\mathbf{r}_{\pi}$。



## 3. 矩阵向量形式

对于有限状态空间和动作空间，由于每一个状态都对应一个贝尔曼方程，因此我们可以将所有状态的贝尔曼方程联立，形成一个线性方程组。通过矩阵和向量的形式来表示，可以更方便地进行计算和求解。

$$\mathbf{v}_{\pi} = \mathbf{r}_{\pi} + \gamma \mathbf{P}_{\pi} \mathbf{v}_{\pi}$$

其中：
- $\mathbf{v}_{\pi}$ 是状态值向量 $[v(s_1), \dots]^T$
- $\mathbf{r}_{\pi}$ 是即时奖励向量 $[r(s_1), \dots]^T$
- $\mathbf{P}_{\pi}$ 是状态转移矩阵 $[P]_{ij} = p(s_j|s_i)$

![](assets-of-强化学习-第二章-状态值与贝尔曼方程/5.png)

## 4. 求解策略值 (Policy Evaluation)

给定策略 $\pi$，在得到其对应的贝尔曼方程后，我们可以从中求解出所有状态值。求解其对应的状态值的过程称为**策略评价**。

### 方法1：解析解
直接利用矩阵求逆：
$$\mathbf{v}_{\pi} = (\mathbf{I} - \gamma \mathbf{P}_{\pi})^{-1}\mathbf{r}_{\pi}$$
- $\mathbf{I} - \gamma \mathbf{P}_{\pi}$ 在 $\gamma < 1$ 时通常是可逆的。
- 缺点：当状态空间巨大时，求逆计算量过大。

### 方法2：数值迭代
从任意初始值 $\mathbf{v}_{0}$ 开始，反复应用贝尔曼方程：
$$\mathbf{v}_{k+1} = \mathbf{r}_{\pi} + \gamma \mathbf{P}_{\pi} \mathbf{v}_{k}$$
序列 $\mathbf{v}_{k}$ 最终会收敛到真实值 $\mathbf{v}_{\pi}$。这正是许多强化学习算法（如值迭代）的基础。

### 示例比较
我们可以通过计算状态值来定量的比较策略的好坏。
- **好策略**：状态值高 (图左，能避开禁区)
- **坏策略**：状态值低 (图右，陷入禁区)

<div style="display: flex; justify-content: space-around;">
    <img src="./assets-of-强化学习-第二章-状态值与贝尔曼方程/6.png" width="45%">
    <img src="./assets-of-强化学习-第二章-状态值与贝尔曼方程/7.png" width="45%">
</div>

## 5. 行动值 (Action Value)

除了评估状态，有时我们需要评估“在状态 $s$ 采取动作 $a$”这个决策的价值，这就是**行动值** $q_\pi(s,a)$。

$$q_\pi(s,a)=\mathbb{E}[G_t|S_t=s,A_t=a]$$

### 状态值与行动值的关系
二者是同一枚硬币的两面：

1. **从行动值到状态值**：
   状态值是行动值的加权平均（对策略求期望）。
   $$v_\pi(s)=\sum_{a\in\mathcal{A}}\pi(a|s)q_\pi(s,a)$$

2. **从状态值到行动值**：
   行动值是即时奖励加上下一步状态值的折扣期望。
   $$q_\pi(s,a)=\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)v_\pi(s^{\prime})$$

![](assets-of-强化学习-第二章-状态值与贝尔曼方程/8.png)

> **常见误区**
> 即使策略 $\pi$ 在 $s$ 也就不会选择动作 $a$（即 $\pi(a|s)=0$），行动值 $q_\pi(s,a)$ 依然存在且有意义。它代表了“**假如**我强行试了一下 $a$，然后继续按原策略 $\pi$ 走，会得到的价值”。这是**探索 (Exploration)** 的基础。

## 6. 总结

| 概念 | 符号 | 核心意义 |
| :--- | :---: | :--- |
| **状态值** | $v_\pi(s)$ | **预测未来**：在这个位置，按照这个打法，平均能得多少分？ |
| **贝尔曼方程** | $v = R + \gamma P v$ | **自洽性**：现在的价值 = 现在的甜头 + 未来的价值 |
| **行动值** | $q_\pi(s,a)$ | **决策评估**：在这个位置，这一步走 $a$，平均能得多少分？ |
| **自举** | Bootstrapping | **迭代求解**：未知的价值依赖于未知的价值，通过迭代逼近真实解 |
