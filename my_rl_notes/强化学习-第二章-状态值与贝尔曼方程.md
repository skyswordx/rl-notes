# 强化学习-第二章-状态值与贝尔曼方程

第二章将介绍一个核心概念和一个核心工具。核心概念是状态值，它是一个评价策略好坏的重要指标。既然状态值这么重要，那么我们应该如何分析它呢？答案就是核心工具**贝尔曼方程**(Bellman equation)。贝尔曼方程描述了所有状态值之间的关系。通过求解贝尔曼方程，我们就可以得到状态值，进而评价一个策略的好坏。

![](assets-of-强化学习-第二章-状态值与贝尔曼方程/chapter-2.png)

## 1. 为什么回报很重要?

上一章介绍了回报的概念，它在强化学习中扮演着重要的角色，因为它可以评估一个策略的好坏。

通过下图的例子，不同策略在 $s_1$ 的选择导致了不同的轨迹。直观上，能避开禁区的策略显然更好。数学上，通过计算回报（Return），我们可以量化这种“好坏”。

- **策略 1 (左)**: 避开禁区，回报最大。
- **策略 2 (中)**: 进入禁区，回报最小。
- **策略 3 (右)**: 50% 概率进入禁区，回报居中。

这证明了：**回报可以被用来评估策略的好坏**。

![](assets-of-强化学习-第二章-状态值与贝尔曼方程/1.png)

## 2. 如何计算回报？

计算回报有两种方法：

- **定义法**：回报等于沿轨迹收集的所有奖励的折扣总和。
  $$v_{1}=r_1+\gamma r_2+\gamma^2r_3+\ldots$$

- **自举法 (Bootstrapping)**：由定义推导，可以用自举重写。
  $$v_{1}=r_1+\gamma(r_2+\gamma r_3+\ldots)=r_1+\gamma v_2$$

自举法揭示了一个核心思想：
- **依赖性**：从一个状态出发的回报值依赖于从其他状态出发获得的回报。
- **贝尔曼方程的雏形**：未知的价值即是自身的函数。

![](assets-of-强化学习-第二章-状态值与贝尔曼方程/2.png)

## 3. 状态值 (State Value)

回报并不适用于一般化的随机情况，因为从一个状态出发可能会得到不同的轨迹和回报。为此，我们引入**状态值**。

**状态值函数** $v_\pi(s)$ 定义为在状态 $s$ 下，遵循策略 $\pi$ 时的预期回报：

$$v_\pi(s) = \mathbb{E}_\pi[G_t|S_t=s]$$

> **核心理解**
> 
> - **随机性来源**: $S_t \xrightarrow{A_t} S_{t+1}, R_{t+1} \dots$ 每一步的状态和即时回报都是随机的，所以 $G_t$ 也是随机变量。
> - **期望的意义**: 因为 $G_t$ 是随机的，我们无法用它直接评估，所以取其**期望** $v_\pi(s)$。
> - **依赖关系**:
> 	- 依赖于 $s$ (不同起点前景不同)
> 	- 依赖于 $\pi$ (不同打法轨迹不同)
> 	- **不**依赖于 $t$ (平稳系统，马尔可夫性)

## 4. 贝尔曼方程 (Bellman Equation)

状态值函数 $v_\pi(s)$ 可以用自举法重写，得到的方程称为贝尔曼方程：

$$
\begin{aligned}
v_{\pi}(s) &= \mathbb{E}[G_t | S_t = s] \\
&= \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
&= \mathbb{E}[R_{t+1} | S_t = s] + \gamma \mathbb{E}[G_{t+1} | S_t = s] \\
&= \underbrace{\sum_{a \in \mathcal{A}} \pi(a|s) \sum_{r \in \mathcal{R}} p(r|s, a)r}_{\text{即时奖励的期望}} + \underbrace{\gamma \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{s' \in \mathcal{S}} p(s'|s, a)v_{\pi}(s')}_{\text{未来奖励的期望}} \\
&= \sum_{a \in \mathcal{A}} \pi(a|s) \left[ \sum_{r \in \mathcal{R}} p(r|s, a)r + \gamma \sum_{s' \in \mathcal{S}} p(s'|s, a)v_{\pi}(s') \right]
\end{aligned}
$$

### 理解方程中的随机性分解

这个方程极其精妙地把**两层随机性**解耦了：

1.  **策略的随机性** ($\pi(a|s)$):
    -   外面的求和 $\sum_{a \in \mathcal{A}}\pi (a | s)$ 是对该状态下所有可能动作的加权平均。
    
2.  **环境(模型)的随机性** ($p(r|s, a)$ 和 $p(s'|s, a)$):
    -   中括号里是对“采取某个具体动作 $a$”后的后果评估。
    -   $\sum p(r|s, a)r$ 是动作 $a$ 带来的**即时奖励**期望。
    -   $\sum p(s'|s, a)v_{\pi}(s')$ 是动作 $a$ 带来的**未来价值**期望。

### 三种等价形式

为了方便推导，贝尔曼方程常写成以下形式：

1.  **贝尔曼期望方程** (利用期望算子):
    $$v_{\pi}(s)=\mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_t=s]$$
2.  **全概率展开形式** (最常用):
    $$v_{\pi}(s)=\sum_{a\in \mathcal{A}} \pi(a|s) \sum_{s'\in \mathcal{S}} \sum_{r\in \mathcal{R}} p(s',r|s,a) [r + \gamma v_{\pi}(s')]$$
3.  **矩阵-向量形式的简记**:
    $$v_{\pi}(s) = r_{\pi}(s) + \gamma \sum_{s' \in \mathcal{S}} p_{\pi}(s'|s)v_{\pi}(s')$$
    -   即时奖励期望: $r_{\pi}(s) \doteq \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{r \in \mathcal{R}} p(r|s, a)r$
    -   状态转移概率: $p_{\pi}(s'|s) \doteq \sum_{a \in \mathcal{A}} \pi(a|s)p(s'|s, a)$

## 5. 矩阵向量形式

对于有限状态空间，我们可以将所有状态的方程联立，形成线性方程组：

$$\mathbf{v}_{\pi} = \mathbf{r}_{\pi} + \gamma \mathbf{P}_{\pi} \mathbf{v}_{\pi}$$

其中：
- $\mathbf{v}_{\pi}$ 是状态值向量 $[v(s_1), \dots]^T$
- $\mathbf{r}_{\pi}$ 是即时奖励向量 $[r(s_1), \dots]^T$
- $\mathbf{P}_{\pi}$ 是状态转移矩阵 $[P]_{ij} = p(s_j|s_i)$

![](assets-of-强化学习-第二章-状态值与贝尔曼方程/5.png)

## 6. 求解策略值 (Policy Evaluation)

给定策略 $\pi$，求解其对应的状态值的过程称为**策略评价**。

### 方法1：解析解
直接利用矩阵求逆：
$$\mathbf{v}_{\pi} = (\mathbf{I} - \gamma \mathbf{P}_{\pi})^{-1}\mathbf{r}_{\pi}$$
- $\mathbf{I} - \gamma \mathbf{P}_{\pi}$ 在 $\gamma < 1$ 时通常是可逆的。
- 缺点：当状态空间巨大时，求逆计算量过大。

### 方法2：数值迭代
从任意初始值 $\mathbf{v}_{0}$ 开始，反复应用贝尔曼方程：
$$\mathbf{v}_{k+1} = \mathbf{r}_{\pi} + \gamma \mathbf{P}_{\pi} \mathbf{v}_{k}$$
序列 $\mathbf{v}_{k}$ 最终会收敛到真实值 $\mathbf{v}_{\pi}$。这正是许多强化学习算法（如值迭代）的基础。

### 示例比较
我们可以通过计算状态值来定量的比较策略的好坏。
- **好策略**：状态值高 (图左，能避开禁区)
- **坏策略**：状态值低 (图右，陷入禁区)

<div style="display: flex; justify-content: space-around;">
    <img src="./assets-of-强化学习-第二章-状态值与贝尔曼方程/6.png" width="45%">
    <img src="./assets-of-强化学习-第二章-状态值与贝尔曼方程/7.png" width="45%">
</div>

## 7. 行动值 (Action Value)

除了评估状态，有时我们需要评估“在状态 $s$ 采取动作 $a$”这个决策的价值，这就是**行动值** $q_\pi(s,a)$。

$$q_\pi(s,a)=\mathbb{E}[G_t|S_t=s,A_t=a]$$

### 状态值与行动值的关系
二者是同一枚硬币的两面：

1. **从行动值到状态值**：
   状态值是行动值的加权平均（对策略求期望）。
   $$v_\pi(s)=\sum_{a\in\mathcal{A}}\pi(a|s)q_\pi(s,a)$$

2. **从状态值到行动值**：
   行动值是即时奖励加上下一步状态值的折扣期望。
   $$q_\pi(s,a)=\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)v_\pi(s^{\prime})$$

![](assets-of-强化学习-第二章-状态值与贝尔曼方程/8.png)

> **常见误区**
> 即使策略 $\pi$ 在 $s$ 也就不会选择动作 $a$（即 $\pi(a|s)=0$），行动值 $q_\pi(s,a)$ 依然存在且有意义。它代表了“**假如**我强行试了一下 $a$，然后继续按原策略 $\pi$ 走，会得到的价值”。这是**探索 (Exploration)** 的基础。

## 8. 总结

| 概念 | 符号 | 核心意义 |
| :--- | :---: | :--- |
| **状态值** | $v_\pi(s)$ | **预测未来**：在这个位置，按照这个打法，平均能得多少分？ |
| **贝尔曼方程** | $v = R + \gamma P v$ | **自洽性**：现在的价值 = 现在的甜头 + 未来的价值 |
| **行动值** | $q_\pi(s,a)$ | **决策评估**：在这个位置，这一步走 $a$，平均能得多少分？ |
| **自举** | Bootstrapping | **迭代求解**：未知的价值依赖于未知的价值，通过迭代逼近真实解 |
